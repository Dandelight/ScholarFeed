[
  {
    "id": "2512.05117v1",
    "title": "The Universal Weight Subspace Hypothesis",
    "authors": [
      "Prakhar Kaushik",
      "Shravan Chaudhari",
      "Ankit Vaidya",
      "Rama Chellappa",
      "Alan Yuille"
    ],
    "summary": "We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05117v1"
  },
  {
    "id": "2512.05112v1",
    "title": "DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation",
    "authors": [
      "Dongzhi Jiang",
      "Renrui Zhang",
      "Haodong Li",
      "Zhuofan Zong",
      "Ziyu Guo",
      "Jun He",
      "Claire Guo",
      "Junyan Ye",
      "Rongyao Fang",
      "Weijia Li",
      "Rui Liu",
      "Hongsheng Li"
    ],
    "summary": "Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05112v1"
  },
  {
    "id": "2512.05110v1",
    "title": "ShadowDraw: From Any Object to Shadow-Drawing Compositional Art",
    "authors": [
      "Rundong Luo",
      "Noah Snavely",
      "Wei-Chiu Ma"
    ],
    "summary": "We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.GR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05110v1"
  },
  {
    "id": "2512.05105v1",
    "title": "Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning",
    "authors": [
      "Purbesh Mitra",
      "Sennur Ulukus"
    ],
    "summary": "Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \\textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "cs.LG",
      "eess.SP"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05105v1"
  },
  {
    "id": "2512.05103v1",
    "title": "TV2TV: A Unified Framework for Interleaved Language and Video Generation",
    "authors": [
      "Xiaochuang Han",
      "Youssef Emad",
      "Melissa Hall",
      "John Nguyen",
      "Karthik Padthe",
      "Liam Robbins",
      "Amir Bar",
      "Delong Chen",
      "Michal Drozdzal",
      "Maha Elbayad",
      "Yushi Hu",
      "Shang-Wen Li",
      "Sreya Dutta Roy",
      "Jakob Verbeek",
      "XuDong Wang",
      "Marjan Ghazvininejad",
      "Luke Zettlemoyer",
      "Emily Dinan"
    ],
    "summary": "Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to \"think in words\" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05103v1"
  },
  {
    "id": "2512.05100v1",
    "title": "Structured Document Translation via Format Reinforcement Learning",
    "authors": [
      "Haiyue Song",
      "Johannes Eschbach-Dymanus",
      "Hour Kaing",
      "Sumire Honda",
      "Hideki Tanaka",
      "Bianka Buschbeck",
      "Masao Utiyama"
    ],
    "summary": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05100v1"
  },
  {
    "id": "2512.05098v1",
    "title": "SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards",
    "authors": [
      "Yuan Gao",
      "Jin Song"
    ],
    "summary": "In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05098v1"
  },
  {
    "id": "2512.05073v1",
    "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
    "authors": [
      "Shashwat Shankar",
      "Subhranshu Pandey",
      "Innocent Dengkhw Mochahari",
      "Bhabesh Mali",
      "Animesh Basak Chowdhury",
      "Sukanta Bhattacharjee",
      "Chandan Karfa"
    ],
    "summary": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR",
      "cs.SE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05073v1"
  },
  {
    "id": "2512.05066v1",
    "title": "Multi-LLM Collaboration for Medication Recommendation",
    "authors": [
      "Huascar Sanchez",
      "Briland Hitaj",
      "Jules Bergmann",
      "Linda Briesemeister"
    ],
    "summary": "As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05066v1"
  },
  {
    "id": "2512.05058v1",
    "title": "Meta-Learning for Quantum Optimization via Quantum Sequence Model",
    "authors": [
      "Yu-Cheng Lin",
      "Yu-Chao Hsu",
      "Samuel Yen-Chi Chen"
    ],
    "summary": "The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a \"learning to learn\" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.",
    "published": "2025-12-04",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05058v1"
  },
  {
    "id": "2512.05049v1",
    "title": "QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory",
    "authors": [
      "Yu-Chao Hsu",
      "Jiun-Cheng Jiang",
      "Chun-Hua Lin",
      "Kuo-Chung Peng",
      "Nan-Yow Chen",
      "Samuel Yen-Chi Chen",
      "En-Jui Kuo",
      "Hsi-Sheng Goan"
    ],
    "summary": "Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.",
    "published": "2025-12-04",
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05049v1"
  },
  {
    "id": "2512.05033v1",
    "title": "Arbitrage: Efficient Reasoning via Advantage-Aware Speculation",
    "authors": [
      "Monishwaran Maheswaran",
      "Rishabh Tiwari",
      "Yuezhou Hu",
      "Kerem Dilmen",
      "Coleman Hooper",
      "Haocheng Xi",
      "Nicholas Lee",
      "Mehrdad Farajtabar",
      "Michael W. Mahoney",
      "Kurt Keutzer",
      "Amir Gholami"
    ],
    "summary": "Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\\sim2\\times$ at matched accuracy.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05033v1"
  },
  {
    "id": "2512.05024v1",
    "title": "Model-Free Assessment of Simulator Fidelity via Quantile Curves",
    "authors": [
      "Garud Iyengar",
      "Yu-Shiou Willy Lin",
      "Kaizheng Wang"
    ],
    "summary": "Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.",
    "published": "2025-12-04",
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05024v1"
  },
  {
    "id": "2512.05013v1",
    "title": "Detecting Perspective Shifts in Multi-agent Systems",
    "authors": [
      "Eric Bridgeford",
      "Hayden Helm"
    ],
    "summary": "Generative models augmented with external tools and update mechanisms (or \\textit{agents}) have demonstrated capabilities beyond intelligent prompting of base models. As agent use proliferates, dynamic multi-agent systems have naturally emerged. Recent work has investigated the theoretical and empirical properties of low-dimensional representations of agents based on query responses at a single time point. This paper introduces the Temporal Data Kernel Perspective Space (TDKPS), which jointly embeds agents across time, and proposes several novel hypothesis tests for detecting behavioral change at the agent- and group-level in black-box multi-agent systems. We characterize the empirical properties of our proposed tests, including their sensitivity to key hyperparameters, in simulations motivated by a multi-agent system of evolving digital personas. Finally, we demonstrate via natural experiment that our proposed tests detect changes that correlate sensitively, specifically, and significantly with a real exogenous event. As far as we are aware, TDKPS is the first principled framework for monitoring behavioral dynamics in black-box multi-agent systems -- a critical capability as generative agent deployment continues to scale.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.MA",
      "stat.ME"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05013v1"
  },
  {
    "id": "2512.05000v1",
    "title": "Reflection Removal through Efficient Adaptation of Diffusion Transformers",
    "authors": [
      "Daniyar Zakarin",
      "Thiemo Wandel",
      "Anton Obukhov",
      "Dengxin Dai"
    ],
    "summary": "We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.05000v1"
  },
  {
    "id": "2512.04992v1",
    "title": "Evolutionary Architecture Search through Grammar-Based Sequence Alignment",
    "authors": [
      "Adri Gómez Martín",
      "Felix Möller",
      "Steven McDonagh",
      "Monica Abella",
      "Manuel Desco",
      "Elliot J. Crowley",
      "Aaron Klein",
      "Linus Ericsson"
    ],
    "summary": "Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.",
    "published": "2025-12-04",
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04992v1"
  },
  {
    "id": "2512.04988v1",
    "title": "Strategic Self-Improvement for Competitive Agents in AI Labour Markets",
    "authors": [
      "Christopher Chiu",
      "Simpson Zhang",
      "Mihaela van der Schaar"
    ],
    "summary": "As artificial intelligence (AI) agents are deployed across economic domains, understanding their strategic behavior and market-level impact becomes critical. This paper puts forward a groundbreaking new framework that is the first to capture the real-world economic forces that shape agentic labor markets: adverse selection, moral hazard, and reputation dynamics. Our framework encapsulates three core capabilities that successful LLM-agents will need: \\textbf{metacognition} (accurate self-assessment of skills), \\textbf{competitive awareness} (modeling rivals and market dynamics), and \\textbf{long-horizon strategic planning}. We illustrate our framework through a tractable simulated gig economy where agentic Large Language Models (LLMs) compete for jobs, develop skills, and adapt their strategies under competitive pressure. Our simulations illustrate how LLM agents explicitly prompted with reasoning capabilities learn to strategically self-improve and demonstrate superior adaptability to changing market conditions. At the market level, our simulations reproduce classic macroeconomic phenomena found in human labor markets, while controlled experiments reveal potential AI-driven economic trends, such as rapid monopolization and systemic price deflation. This work provides a foundation to further explore the economic properties of AI-driven labour markets, and a conceptual framework to study the strategic reasoning capabilities in agents competing in the emerging economy.",
    "published": "2025-12-04",
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04988v1"
  },
  {
    "id": "2512.04967v1",
    "title": "Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis",
    "authors": [
      "Jasmaine Khale",
      "Ravi Prakash Srivastava"
    ],
    "summary": "Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04967v1"
  },
  {
    "id": "2512.04963v1",
    "title": "GeoPE:A Unified Geometric Positional Embedding for Structured Tensors",
    "authors": [
      "Yupu Yao",
      "Bowen Yang"
    ],
    "summary": "Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04963v1"
  },
  {
    "id": "2512.04958v1",
    "title": "Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning",
    "authors": [
      "Roberto Cipollone",
      "Luca Iocchi",
      "Matteo Leonetti"
    ],
    "summary": "The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04958v1"
  },
  {
    "id": "2512.04957v1",
    "title": "LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics",
    "authors": [
      "Weiye Shi",
      "Zhaowei Zhang",
      "Shaoheng Yan",
      "Yaodong Yang"
    ],
    "summary": "Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04957v1"
  },
  {
    "id": "2512.04949v1",
    "title": "CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent",
    "authors": [
      "Leyang Shen",
      "Yang Zhang",
      "Chun Kai Ling",
      "Xiaoyan Zhao",
      "Tat-Seng Chua"
    ],
    "summary": "Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04949v1"
  },
  {
    "id": "2512.04938v1",
    "title": "Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases",
    "authors": [
      "Raquel Norel",
      "Michele Merler",
      "Pavitra Modi"
    ],
    "summary": "Patients with rare neurological diseases report cognitive symptoms -\"brain fog\"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived \"Proficiency in Verbal Discourse\" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04938v1"
  },
  {
    "id": "2512.04923v1",
    "title": "Algorithmic Thinking Theory",
    "authors": [
      "MohammadHossein Bateni",
      "Vincent Cohen-Addad",
      "Yuzhou Gu",
      "Silvio Lattanzi",
      "Simon Meierhans",
      "Christopher Mohri"
    ],
    "summary": "Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.\n  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04923v1"
  },
  {
    "id": "2512.04921v1",
    "title": "The AI Consumer Index (ACE)",
    "authors": [
      "Julien Benchek",
      "Rohit Shetty",
      "Benjamin Hunsberger",
      "Ajay Arun",
      "Zach Richards",
      "Brendan Foody",
      "Osvald Nitski",
      "Bertie Vidgen"
    ],
    "summary": "We introduce the first version of the AI Consumer Index (ACE), a benchmark for assessing whether frontier AI models can perform high-value consumer tasks. ACE contains a hidden heldout set of 400 test cases, split across four consumer activities: shopping, food, gaming, and DIY. We are also open sourcing 80 cases as a devset with a CC-BY license. For the ACE leaderboard we evaluated 10 frontier models (with websearch turned on) using a novel grading methodology that dynamically checks whether relevant parts of the response are grounded in the retrieved web sources. GPT 5 (Thinking = High) is the top-performing model, scoring 56.1%, followed by o3 Pro (Thinking = On) (55.2%) and GPT 5.1 (Thinking = High) (55.1%). Models differ across domains, and in Shopping the top model scores under 50%. For some requests (such as giving the correct price or providing working links), models are highly prone to hallucination. Overall, ACE shows a substantial gap between the performance of even the best models and consumers' AI needs.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04921v1"
  },
  {
    "id": "2512.04910v1",
    "title": "Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming",
    "authors": [
      "Fang Li"
    ],
    "summary": "This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.",
    "published": "2025-12-04",
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04910v1"
  },
  {
    "id": "2512.04904v1",
    "title": "ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching",
    "authors": [
      "Guanbo Huang",
      "Jingjia Mao",
      "Fanding Huang",
      "Fengkai Liu",
      "Xiangyang Luo",
      "Yaoyuan Liang",
      "Jiasheng Lu",
      "Xiaoe Wang",
      "Pei Liu",
      "Ruiliu Fu",
      "Shao-Lun Huang"
    ],
    "summary": "Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04904v1"
  },
  {
    "id": "2512.04895v1",
    "title": "Chameleon: Adaptive Adversarial Agents for Scaling-Based Visual Prompt Injection in Multimodal AI Systems",
    "authors": [
      "M Zeeshan",
      "Saud Satti"
    ],
    "summary": "Multimodal Artificial Intelligence (AI) systems, particularly Vision-Language Models (VLMs), have become integral to critical applications ranging from autonomous decision-making to automated document processing. As these systems scale, they rely heavily on preprocessing pipelines to handle diverse inputs efficiently. However, this dependency on standard preprocessing operations, specifically image downscaling, creates a significant yet often overlooked security vulnerability. While intended for computational optimization, scaling algorithms can be exploited to conceal malicious visual prompts that are invisible to human observers but become active semantic instructions once processed by the model. Current adversarial strategies remain largely static, failing to account for the dynamic nature of modern agentic workflows. To address this gap, we propose Chameleon, a novel, adaptive adversarial framework designed to expose and exploit scaling vulnerabilities in production VLMs. Unlike traditional static attacks, Chameleon employs an iterative, agent-based optimization mechanism that dynamically refines image perturbations based on the target model's real-time feedback. This allows the framework to craft highly robust adversarial examples that survive standard downscaling operations to hijack downstream execution. We evaluate Chameleon against Gemini 2.5 Flash model. Our experiments demonstrate that Chameleon achieves an Attack Success Rate (ASR) of 84.5% across varying scaling factors, significantly outperforming static baseline attacks which average only 32.1%. Furthermore, we show that these attacks effectively compromise agentic pipelines, reducing decision-making accuracy by over 45% in multi-step tasks. Finally, we discuss the implications of these vulnerabilities and propose multi-scale consistency checks as a necessary defense mechanism.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04895v1"
  },
  {
    "id": "2512.04871v1",
    "title": "STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions",
    "authors": [
      "Junjie Fan",
      "Hongye Zhao",
      "Linduo Wei",
      "Jiayu Rao",
      "Guijia Li",
      "Jiaxin Yuan",
      "Wenqi Xu",
      "Yong Qi"
    ],
    "summary": "Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04871v1"
  },
  {
    "id": "2512.04869v1",
    "title": "Developing a General Personal Tutor for Education",
    "authors": [
      "Jaan Aru",
      "Kristjan-Julius Laak"
    ],
    "summary": "The vision of a universal AI tutor has remained elusive, despite decades of effort. Could LLMs be the game-changer? We overview novel issues arising from developing a nationwide AI tutor. We highlight the practical questions that point to specific gaps in our scientific understanding of the learning process.",
    "published": "2025-12-04",
    "categories": [
      "cs.CY",
      "cs.AI",
      "q-bio.NC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04869v1"
  },
  {
    "id": "2512.04868v1",
    "title": "SEAL: Self-Evolving Agentic Learning for Conversational Question Answering over Knowledge Graphs",
    "authors": [
      "Hao Wang",
      "Jialun Zhong",
      "Changcheng Wang",
      "Zhujun Nie",
      "Zheng Li",
      "Shunyu Yao",
      "Yanzeng Li",
      "Xinchi Li"
    ],
    "summary": "Knowledge-based conversational question answering (KBCQA) confronts persistent challenges in resolving coreference, modeling contextual dependencies, and executing complex logical reasoning. Existing approaches, whether end-to-end semantic parsing or stepwise agent-based reasoning, often suffer from structural inaccuracies and prohibitive computational costs, particularly when processing intricate queries over large knowledge graphs. To address these limitations, we introduce SEAL, a novel two-stage semantic parsing framework grounded in self-evolving agentic learning. In the first stage, a large language model (LLM) extracts a minimal S-expression core that captures the essential semantics of the input query. This core is then refined by an agentic calibration module, which corrects syntactic inconsistencies and aligns entities and relations precisely with the underlying knowledge graph. The second stage employs template-based completion, guided by question-type prediction and placeholder instantiation, to construct a fully executable S-expression. This decomposition not only simplifies logical form generation but also significantly enhances structural fidelity and linking efficiency. Crucially, SEAL incorporates a self-evolving mechanism that integrates local and global memory with a reflection module, enabling continuous adaptation from dialog history and execution feedback without explicit retraining. Extensive experiments on the SPICE benchmark demonstrate that SEAL achieves state-of-the-art performance, especially in multi-hop reasoning, comparison, and aggregation tasks. The results validate notable gains in both structural accuracy and computational efficiency, underscoring the framework's capacity for robust and scalable conversational reasoning.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04868v1"
  },
  {
    "id": "2512.04864v1",
    "title": "Are Your Agents Upward Deceivers?",
    "authors": [
      "Dadi Guo",
      "Qingyu Liu",
      "Dongrui Liu",
      "Qihan Ren",
      "Shuai Shao",
      "Tianyi Qiu",
      "Haoran Li",
      "Yi R. Fung",
      "Zhongjie Ba",
      "Juntao Dai",
      "Jiaming Ji",
      "Zhikai Chen",
      "Jialing Tao",
      "Yaodong Yang",
      "Jing Shao",
      "Xia Hu"
    ],
    "summary": "Large Language Model (LLM)-based agents are increasingly used as autonomous subordinates that carry out tasks for users. This raises the question of whether they may also engage in deception, similar to how individuals in human organizations lie to superiors to create a good image or avoid punishment. We observe and define agentic upward deception, a phenomenon in which an agent facing environmental constraints conceals its failure and performs actions that were not requested without reporting. To assess its prevalence, we construct a benchmark of 200 tasks covering five task types and eight realistic scenarios in a constrained environment, such as broken tools or mismatched information sources. Evaluations of 11 popular LLMs reveal that these agents typically exhibit action-based deceptive behaviors, such as guessing results, performing unsupported simulations, substituting unavailable information sources, and fabricating local files. We further test prompt-based mitigation and find only limited reductions, suggesting that it is difficult to eliminate and highlighting the need for stronger mitigation strategies to ensure the safety of LLM-based agents.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04864v1"
  },
  {
    "id": "2512.04854v1",
    "title": "From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research",
    "authors": [
      "Lukas Weidener",
      "Marko Brkić",
      "Chiara Bacci",
      "Mihailo Jovanović",
      "Emre Ulgac",
      "Alex Dobrin",
      "Johannes Weniger",
      "Martin Vlas",
      "Ritvik Singh",
      "Aakaash Meduri"
    ],
    "summary": "Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04854v1"
  },
  {
    "id": "2512.04847v1",
    "title": "Language Models as Semantic Teachers: Post-Training Alignment for Medical Audio Understanding",
    "authors": [
      "Tsai-Ning Wang",
      "Lin-Lin Chen",
      "Neil Zeghidour",
      "Aaqib Saeed"
    ],
    "summary": "Pre-trained audio models excel at detecting acoustic patterns in auscultation sounds but often fail to grasp their clinical significance, limiting their use and performance in diagnostic tasks. To bridge this gap, we introduce AcuLa (Audio-Clinical Understanding via Language Alignment), a lightweight post-training framework that instills semantic understanding into any audio encoder by aligning it with a medical language model, which acts as a \"semantic teacher.\" To enable alignment at scale, we construct a large-scale dataset by leveraging off-the-shelf large language models to translate the rich, structured metadata accompanying existing audio recordings into coherent clinical reports. Our alignment strategy combines a representation-level contrastive objective with a self-supervised modeling, ensuring that the model learns clinical semantics while preserving fine-grained temporal cues. AcuLa achieves state-of-the-art results across 18 diverse cardio-respiratory tasks from 10 different datasets, improving the mean AUROC on classification benchmarks from 0.68 to 0.79 and, on the most challenging COVID-19 cough detection task, boosting the AUROC from 0.55 to 0.89. Our work demonstrates that this audio-language alignment transforms purely acoustic models into clinically-aware diagnostic tools, establishing a novel paradigm for enhancing physiological understanding in audio-based health monitoring.",
    "published": "2025-12-04",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04847v1"
  },
  {
    "id": "2512.04844v1",
    "title": "Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates",
    "authors": [
      "Atsuki Yamaguchi",
      "Terufumi Morishita",
      "Aline Villavicencio",
      "Nikolaos Aletras"
    ],
    "summary": "Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04844v1"
  },
  {
    "id": "2512.04843v1",
    "title": "From Symptoms to Systems: An Expert-Guided Approach to Understanding Risks of Generative AI for Eating Disorders",
    "authors": [
      "Amy Winecoff",
      "Kevin Klyman"
    ],
    "summary": "Generative AI systems may pose serious risks to individuals vulnerable to eating disorders. Existing safeguards tend to overlook subtle but clinically significant cues, leaving many risks unaddressed. To better understand the nature of these risks, we conducted semi-structured interviews with 15 clinicians, researchers, and advocates with expertise in eating disorders. Using abductive qualitative analysis, we developed an expert-guided taxonomy of generative AI risks across seven categories: (1) providing generalized health advice; (2) encouraging disordered behaviors; (3) supporting symptom concealment; (4) creating thinspiration; (5) reinforcing negative self-beliefs; (6) promoting excessive focus on the body; and (7) perpetuating narrow views about eating disorders. Our results demonstrate how certain user interactions with generative AI systems intersect with clinical features of eating disorders in ways that may intensify risk. We discuss implications of our work, including approaches for risk assessment, safeguard design, and participatory evaluation practices with domain experts.",
    "published": "2025-12-04",
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04843v1"
  },
  {
    "id": "2512.04841v1",
    "title": "SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security",
    "authors": [
      "Wei Zhao",
      "Zhe Li",
      "Jun Sun"
    ],
    "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but remain vulnerable to adversarial manipulations such as jailbreaking, where crafted prompts bypass safety mechanisms. Understanding the causal factors behind such vulnerabilities is essential for building reliable defenses.\n  In this work, we introduce a unified causality analysis framework that systematically supports all levels of causal investigation in LLMs, ranging from token-level, neuron-level, and layer-level interventions to representation-level analysis. The framework enables consistent experimentation and comparison across diverse causality-based attack and defense methods. Accompanying this implementation, we provide the first comprehensive survey of causality-driven jailbreak studies and empirically evaluate the framework on multiple open-weight models and safety-critical benchmarks including jailbreaks, hallucination detection, backdoor identification, and fairness evaluation. Our results reveal that: (1) targeted interventions on causally critical components can reliably modify safety behavior; (2) safety-related mechanisms are highly localized (i.e., concentrated in early-to-middle layers with only 1--2\\% of neurons exhibiting causal influence); and (3) causal features extracted from our framework achieve over 95\\% detection accuracy across multiple threat types.\n  By bridging theoretical causality analysis and practical model safety, our framework establishes a reproducible foundation for research on causality-based attacks, interpretability, and robust attack detection and mitigation in LLMs. Code is available at https://github.com/Amadeuszhao/SOK_Casuality.",
    "published": "2025-12-04",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04841v1"
  },
  {
    "id": "2512.04834v1",
    "title": "Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case",
    "authors": [
      "Vignesh Kumar Kembu",
      "Pierandrea Morandini",
      "Marta Bianca Maria Ranzini",
      "Antonino Nocera"
    ],
    "summary": "Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04834v1"
  },
  {
    "id": "2512.04829v1",
    "title": "Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing",
    "authors": [
      "Rasul Tutunov",
      "Alexandre Maraval",
      "Antoine Grosnit",
      "Xihan Li",
      "Jun Wang",
      "Haitham Bou-Ammar"
    ],
    "summary": "Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04829v1"
  },
  {
    "id": "2512.04822v1",
    "title": "Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions",
    "authors": [
      "Liam McGee",
      "James Harvey",
      "Lucy Cull",
      "Andreas Vermeulen",
      "Bart-Floris Visscher",
      "Malvika Sharan"
    ],
    "summary": "In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04822v1"
  },
  {
    "id": "2512.04808v1",
    "title": "Setting up for failure: automatic discovery of the neural mechanisms of cognitive errors",
    "authors": [
      "Puria Radmard",
      "Paul M. Bays",
      "Máté Lengyel"
    ],
    "summary": "Discovering the neural mechanisms underpinning cognition is one of the grand challenges of neuroscience. However, previous approaches for building models of RNN dynamics that explain behaviour required iterative refinement of architectures and/or optimisation objectives, resulting in a piecemeal, and mostly heuristic, human-in-the-loop process. Here, we offer an alternative approach that automates the discovery of viable RNN mechanisms by explicitly training RNNs to reproduce behaviour, including the same characteristic errors and suboptimalities, that humans and animals produce in a cognitive task. Achieving this required two main innovations. First, as the amount of behavioural data that can be collected in experiments is often too limited to train RNNs, we use a non-parametric generative model of behavioural responses to produce surrogate data for training RNNs. Second, to capture all relevant statistical aspects of the data, we developed a novel diffusion model-based approach for training RNNs. To showcase the potential of our approach, we chose a visual working memory task as our test-bed, as behaviour in this task is well known to produce response distributions that are patently multimodal (due to swap errors). The resulting network dynamics correctly qualitative features of macaque neural data. Importantly, these results were not possible to obtain with more traditional approaches, i.e., when only a limited set of behavioural signatures (rather than the full richness of behavioural response distributions) were fitted, or when RNNs were trained for task optimality (instead of reproducing behaviour). Our approach also yields novel predictions about the mechanism of swap errors, which can be readily tested in experiments. These results suggest that fitting RNNs to rich patterns of behaviour provides a powerful way to automatically discover mechanisms of important cognitive functions.",
    "published": "2025-12-04",
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04808v1"
  },
  {
    "id": "2512.04803v1",
    "title": "287,872 Supermassive Black Holes Masses: Deep Learning Approaching Reverberation Mapping Accuracy",
    "authors": [
      "Yuhao Lu",
      "HengJian SiTu",
      "Jie Li",
      "Yixuan Li",
      "Yang Liu",
      "Wenbin Lin",
      "Yu Wang"
    ],
    "summary": "We present a population-scale catalogue of 287,872 supermassive black hole masses with high accuracy. Using a deep encoder-decoder network trained on optical spectra with reverberation-mapping (RM) based labels of 849 quasars and applied to all SDSS quasars up to $z=4$, our method achieves a root-mean-square error of $0.058$\\,dex, a relative uncertainty of $\\approx 14\\%$, and coefficient of determination $R^{2}\\approx0.91$ with respect to RM-based masses, far surpassing traditional single-line virial estimators. Notably, the high accuracy is maintained for both low ($<10^{7.5}\\,M_\\odot$) and high ($>10^{9}\\,M_\\odot$) mass quasars, where empirical relations are unreliable.",
    "published": "2025-12-04",
    "categories": [
      "astro-ph.GA",
      "astro-ph.HE",
      "astro-ph.IM",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04803v1"
  },
  {
    "id": "2512.04797v1",
    "title": "SIMA 2: A Generalist Embodied Agent for Virtual Worlds",
    "authors": [
      "SIMA team",
      "Adrian Bolton",
      "Alexander Lerchner",
      "Alexandra Cordell",
      "Alexandre Moufarek",
      "Andrew Bolt",
      "Andrew Lampinen",
      "Anna Mitenkova",
      "Arne Olav Hallingstad",
      "Bojan Vujatovic",
      "Bonnie Li",
      "Cong Lu",
      "Daan Wierstra",
      "Daniel P. Sawyer",
      "Daniel Slater",
      "David Reichert",
      "Davide Vercelli",
      "Demis Hassabis",
      "Drew A. Hudson",
      "Duncan Williams",
      "Ed Hirst",
      "Fabio Pardo",
      "Felix Hill",
      "Frederic Besse",
      "Hannah Openshaw",
      "Harris Chan",
      "Hubert Soyer",
      "Jane X. Wang",
      "Jeff Clune",
      "John Agapiou",
      "John Reid",
      "Joseph Marino",
      "Junkyung Kim",
      "Karol Gregor",
      "Kaustubh Sridhar",
      "Kay McKinney",
      "Laura Kampis",
      "Lei M. Zhang",
      "Loic Matthey",
      "Luyu Wang",
      "Maria Abi Raad",
      "Maria Loks-Thompson",
      "Martin Engelcke",
      "Matija Kecman",
      "Matthew Jackson",
      "Maxime Gazeau",
      "Ollie Purkiss",
      "Oscar Knagg",
      "Peter Stys",
      "Piermaria Mendolicchio",
      "Raia Hadsell",
      "Rosemary Ke",
      "Ryan Faulkner",
      "Sarah Chakera",
      "Satinder Singh Baveja",
      "Shane Legg",
      "Sheleem Kashem",
      "Tayfun Terzi",
      "Thomas Keck",
      "Tim Harley",
      "Tim Scholtes",
      "Tyson Roberts",
      "Volodymyr Mnih",
      "Yulan Liu",
      "Zhengdong Wang",
      "Zoubin Ghahramani"
    ],
    "summary": "We introduce SIMA 2, a generalist embodied agent that understands and acts in a wide variety of 3D virtual worlds. Built upon a Gemini foundation model, SIMA 2 represents a significant step toward active, goal-directed interaction within an embodied environment. Unlike prior work (e.g., SIMA 1) limited to simple language commands, SIMA 2 acts as an interactive partner, capable of reasoning about high-level goals, conversing with the user, and handling complex instructions given through language and images. Across a diverse portfolio of games, SIMA 2 substantially closes the gap with human performance and demonstrates robust generalization to previously unseen environments, all while retaining the base model's core reasoning capabilities. Furthermore, we demonstrate a capacity for open-ended self-improvement: by leveraging Gemini to generate tasks and provide rewards, SIMA 2 can autonomously learn new skills from scratch in a new environment. This work validates a path toward creating versatile and continuously learning agents for both virtual and, eventually, physical worlds.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04797v1"
  },
  {
    "id": "2512.04793v1",
    "title": "YingMusic-SVC: Real-World Robust Zero-Shot Singing Voice Conversion with Flow-GRPO and Singing-Specific Inductive Biases",
    "authors": [
      "Gongyu Chen",
      "Xiaoyu Zhang",
      "Zhenqiang Weng",
      "Junjie Zheng",
      "Da Shen",
      "Chaofan Ding",
      "Wei-Qiang Zhang",
      "Zihao Chen"
    ],
    "summary": "Singing voice conversion (SVC) aims to render the target singer's timbre while preserving melody and lyrics. However, existing zero-shot SVC systems remain fragile in real songs due to harmony interference, F0 errors, and the lack of inductive biases for singing. We propose YingMusic-SVC, a robust zero-shot framework that unifies continuous pre-training, robust supervised fine-tuning, and Flow-GRPO reinforcement learning. Our model introduces a singing-trained RVC timbre shifter for timbre-content disentanglement, an F0-aware timbre adaptor for dynamic vocal expression, and an energy-balanced rectified flow matching loss to enhance high-frequency fidelity. Experiments on a graded multi-track benchmark show that YingMusic-SVC achieves consistent improvements over strong open-source baselines in timbre similarity, intelligibility, and perceptual naturalness, especially under accompanied and harmony-contaminated conditions, demonstrating its effectiveness for real-world SVC deployment.",
    "published": "2025-12-04",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04793v1"
  },
  {
    "id": "2512.04785v1",
    "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
    "authors": [
      "Eranga Bandara",
      "Amin Hass",
      "Ross Gore",
      "Sachin Shetty",
      "Ravi Mukkamala",
      "Safdar H. Bouk",
      "Xueping Liang",
      "Ng Wee Keong",
      "Kasun De Zoysa",
      "Aruna Withanage",
      "Nilaan Loganathan"
    ],
    "summary": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04785v1"
  },
  {
    "id": "2512.04779v1",
    "title": "YingMusic-Singer: Zero-shot Singing Voice Synthesis and Editing with Annotation-free Melody Guidance",
    "authors": [
      "Junjie Zheng",
      "Chunbo Hao",
      "Guobin Ma",
      "Xiaoyu Zhang",
      "Gongyu Chen",
      "Chaofan Ding",
      "Zihao Chen",
      "Lei Xie"
    ],
    "summary": "Singing Voice Synthesis (SVS) remains constrained in practical deployment due to its strong dependence on accurate phoneme-level alignment and manually annotated melody contours, requirements that are resource-intensive and hinder scalability. To overcome these limitations, we propose a melody-driven SVS framework capable of synthesizing arbitrary lyrics following any reference melody, without relying on phoneme-level alignment. Our method builds on a Diffusion Transformer (DiT) architecture, enhanced with a dedicated melody extraction module that derives melody representations directly from reference audio. To ensure robust melody encoding, we employ a teacher model to guide the optimization of the melody extractor, alongside an implicit alignment mechanism that enforces similarity distribution constraints for improved melodic stability and coherence. Additionally, we refine duration modeling using weakly annotated song data and introduce a Flow-GRPO reinforcement learning strategy with a multi-objective reward function to jointly enhance pronunciation clarity and melodic fidelity. Experiments show that our model achieves superior performance over existing approaches in both objective measures and subjective listening tests, especially in zero-shot and lyric adaptation settings, while maintaining high audio quality without manual annotation. This work offers a practical and scalable solution for advancing data-efficient singing voice synthesis. To support reproducibility, we release our inference code and model checkpoints.",
    "published": "2025-12-04",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04779v1"
  },
  {
    "id": "2512.04773v1",
    "title": "Using Machine Learning to Take Stay-or-Go Decisions in Data-driven Drone Missions",
    "authors": [
      "Giorgos Polychronis",
      "Foivos Pournaropoulos",
      "Christos D. Antonopoulos",
      "Spyros Lalis"
    ],
    "summary": "Drones are becoming indispensable in many application domains. In data-driven missions, besides sensing, the drone must process the collected data at runtime to decide whether additional action must be taken on the spot, before moving to the next point of interest. If processing does not reveal an event or situation that requires such an action, the drone has waited in vain instead of moving to the next point. If, however, the drone starts moving to the next point and it turns out that a follow-up action is needed at the previous point, it must spend time to fly-back. To take this decision, we propose different machine-learning methods based on branch prediction and reinforcement learning. We evaluate these methods for a wide range of scenarios where the probability of event occurrence changes with time. Our results show that the proposed methods consistently outperform the regression-based method proposed in the literature and can significantly improve the worst-case mission time by up to 4.1x. Also, the achieved median mission time is very close, merely up to 2.7% higher, to that of a method with perfect knowledge of the current underlying event probability at each point of interest.",
    "published": "2025-12-04",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04773v1"
  },
  {
    "id": "2512.04770v1",
    "title": "Embodied Co-Design for Rapidly Evolving Agents: Taxonomy, Frontiers, and Challenges",
    "authors": [
      "Yuxing Wang",
      "Zhiyu Chen",
      "Tiantian Zhang",
      "Qiyue Yin",
      "Yongzhe Chang",
      "Zhiheng Li",
      "Liang Wang",
      "Xueqian Wang"
    ],
    "summary": "Brain-body co-evolution enables animals to develop complex behaviors in their environments. Inspired by this biological synergy, embodied co-design (ECD) has emerged as a transformative paradigm for creating intelligent agents-from virtual creatures to physical robots-by jointly optimizing their morphologies and controllers rather than treating control in isolation. This integrated approach facilitates richer environmental interactions and robust task performance. In this survey, we provide a systematic overview of recent advances in ECD. We first formalize the concept of ECD and position it within related fields. We then introduce a hierarchical taxonomy: a lower layer that breaks down agent design into three fundamental components-controlling brain, body morphology, and task environment-and an upper layer that integrates these components into four major ECD frameworks: bi-level, single-level, generative, and open-ended. This taxonomy allows us to synthesize insights from more than one hundred recent studies. We further review notable benchmarks, datasets, and applications in both simulated and real-world scenarios. Finally, we identify significant challenges and offer insights into promising future research directions. A project associated with this survey has been created at https://github.com/Yuxing-Wang-THU/SurveyBrainBody.",
    "published": "2025-12-04",
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.ET",
      "eess.SY"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04770v1"
  },
  {
    "id": "2512.04764v1",
    "title": "Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect",
    "authors": [
      "Dario Pesenti",
      "Alessandro Bogani",
      "Katya Tentori",
      "Stefano Teso"
    ],
    "summary": "Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04764v1"
  },
  {
    "id": "2512.04749v1",
    "title": "UnwrapDiff: Conditional Diffusion for Robust InSAR Phase Unwrapping",
    "authors": [
      "Yijia Song",
      "Juliet Biggs",
      "Alin Achim",
      "Robert Popescu",
      "Simon Orrego",
      "Nantheera Anantrasirichai"
    ],
    "summary": "Phase unwrapping is a fundamental problem in InSAR data processing, supporting geophysical applications such as deformation monitoring and hazard assessment. Its reliability is limited by noise and decorrelation in radar acquisitions, which makes accurate reconstruction of the deformation signal challenging. We propose a denoising diffusion probabilistic model (DDPM)-based framework for InSAR phase unwrapping, UnwrapDiff, in which the output of the traditional minimum cost flow algorithm (SNAPHU) is incorporated as conditional guidance. To evaluate robustness, we construct a synthetic dataset that incorporates atmospheric effects and diverse noise patterns, representative of realistic InSAR observations. Experiments show that the proposed model leverages the conditional prior while reducing the effect of diverse noise patterns, achieving on average a 10.11\\% reduction in NRMSE compared to SNAPHU. It also achieves better reconstruction quality in difficult cases such as dyke intrusions.",
    "published": "2025-12-04",
    "categories": [
      "physics.geo-ph",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04749v1"
  },
  {
    "id": "2512.04746v1",
    "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "authors": [
      "Wenhua Cheng",
      "Weiwei Zhang",
      "Heng Guo",
      "Haihao Shen"
    ],
    "summary": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04746v1"
  },
  {
    "id": "2512.04745v1",
    "title": "Neural Policy Composition from Free Energy Minimization",
    "authors": [
      "Francesca Rossi",
      "Veronica Centorrino",
      "Francesco Bullo",
      "Giovanni Russo"
    ],
    "summary": "The ability to compose acquired skills to plan and execute behaviors is a hallmark of natural intelligence. Yet, despite remarkable cross-disciplinary efforts, a principled account of how task structure shapes gating and how such computations could be delivered in neural circuits, remains elusive. Here we introduce GateMod, an interpretable theoretically grounded computational model linking the emergence of gating to the underlying decision-making task, and to a neural circuit architecture. We first develop GateFrame, a normative framework casting policy gating into the minimization of the free energy. This framework, relating gating rules to task, applies broadly across neuroscience, cognitive and computational sciences. We then derive GateFlow, a continuous-time energy based dynamics that provably converges to GateFrame optimal solution. Convergence, exponential and global, follows from a contractivity property that also yields robustness and other desirable properties. Finally, we derive a neural circuit from GateFlow, GateNet. This is a soft-competitive recurrent circuit whose components perform local and contextual computations consistent with known dendritic and neural processing motifs. We evaluate GateMod across two different settings: collective behaviors in multi-agent systems and human decision-making in multi-armed bandits. In all settings, GateMod provides interpretable mechanistic explanations of gating and quantitatively matches or outperforms established models. GateMod offers a unifying framework for neural policy gating, linking task objectives, dynamical computation, and circuit-level mechanisms. It provides a framework to understand gating in natural agents beyond current explanations and to equip machines with this ability.",
    "published": "2025-12-04",
    "categories": [
      "math.OC",
      "cs.AI",
      "eess.SY",
      "nlin.AO"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04745v1"
  },
  {
    "id": "2512.04738v1",
    "title": "OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models",
    "authors": [
      "Zhuoyue Wan",
      "Wentao Hu",
      "Chen Jason Zhang",
      "Yuanfeng Song",
      "Shuaimin Li",
      "Ruiqiang Xiao",
      "Xiao-Yong Wei",
      "Raymond Chi-Wing Wong"
    ],
    "summary": "Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04738v1"
  },
  {
    "id": "2512.04733v1",
    "title": "E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving",
    "authors": [
      "Yihong Tang",
      "Haicheng Liao",
      "Tong Nie",
      "Junlin He",
      "Ao Qu",
      "Kehua Chen",
      "Wei Ma",
      "Zhenning Li",
      "Lijun Sun",
      "Chengzhong Xu"
    ],
    "summary": "End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04733v1"
  },
  {
    "id": "2512.04728v1",
    "title": "Measuring the Unspoken: A Disentanglement Model and Benchmark for Psychological Analysis in the Wild",
    "authors": [
      "Yigui Feng",
      "Qinglin Wang",
      "Haotian Mo",
      "Yang Liu",
      "Ke Liu",
      "Gencheng Liu",
      "Xinhai Chen",
      "Siqi Shen",
      "Songzhu Mei",
      "Jie Liu"
    ],
    "summary": "Generative psychological analysis of in-the-wild conversations faces two fundamental challenges: (1) existing Vision-Language Models (VLMs) fail to resolve Articulatory-Affective Ambiguity, where visual patterns of speech mimic emotional expressions; and (2) progress is stifled by a lack of verifiable evaluation metrics capable of assessing visual grounding and reasoning depth. We propose a complete ecosystem to address these twin challenges. First, we introduce Multilevel Insight Network for Disentanglement(MIND), a novel hierarchical visual encoder that introduces a Status Judgment module to algorithmically suppress ambiguous lip features based on their temporal feature variance, achieving explicit visual disentanglement. Second, we construct ConvoInsight-DB, a new large-scale dataset with expert annotations for micro-expressions and deep psychological inference. Third, Third, we designed the Mental Reasoning Insight Rating Metric (PRISM), an automated dimensional framework that uses expert-guided LLM to measure the multidimensional performance of large mental vision models. On our PRISM benchmark, MIND significantly outperforms all baselines, achieving a +86.95% gain in micro-expression detection over prior SOTA. Ablation studies confirm that our Status Judgment disentanglement module is the most critical component for this performance leap. Our code has been opened.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04728v1"
  },
  {
    "id": "2512.04727v1",
    "title": "Sequential Enumeration in Large Language Models",
    "authors": [
      "Kuinan Hou",
      "Marco Zorzi",
      "Alberto Testolin"
    ],
    "summary": "Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04727v1"
  },
  {
    "id": "2512.04716v1",
    "title": "Towards an AI Fluid Scientist: LLM-Powered Scientific Discovery in Experimental Fluid Mechanics",
    "authors": [
      "Haodong Feng",
      "Lugang Ye",
      "Dixia Fan"
    ],
    "summary": "The integration of artificial intelligence into experimental fluid mechanics promises to accelerate discovery, yet most AI applications remain narrowly focused on numerical studies. This work proposes an AI Fluid Scientist framework that autonomously executes the complete experimental workflow: hypothesis generation, experimental design, robotic execution, data analysis, and manuscript preparation. We validate this through investigation of vortex-induced vibration (VIV) and wake-induced vibration (WIV) in tandem cylinders. Our work has four key contributions: (1) A computer-controlled circulating water tunnel (CWT) with programmatic control of flow velocity, cylinder position, and forcing parameters (vibration frequency and amplitude) with data acquisition (displacement, force, and torque). (2) Automated experiments reproduce literature benchmarks (Khalak and Williamson [1999] and Assi et al. [2013, 2010]) with frequency lock-in within 4% and matching critical spacing trends. (3) The framework with Human-in-the-Loop (HIL) discovers more WIV amplitude response phenomena, and uses a neural network to fit physical laws from data, which is 31% higher than that of polynomial fitting. (4) The framework with multi-agent with virtual-real interaction system executes hundreds of experiments end-to-end, which automatically completes the entire process of scientific research from hypothesis generation, experimental design, experimental execution, data analysis, and manuscript preparation. It greatly liberates human researchers and improves study efficiency, providing new paradigm for the development and research of experimental fluid mechanics.",
    "published": "2025-12-04",
    "categories": [
      "physics.flu-dyn",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04716v1"
  },
  {
    "id": "2512.04714v1",
    "title": "Playing the Player: A Heuristic Framework for Adaptive Poker AI",
    "authors": [
      "Andrew Paterson",
      "Carl Sanders"
    ],
    "summary": "For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04714v1"
  },
  {
    "id": "2512.04711v1",
    "title": "Large Speech Model Enabled Semantic Communication",
    "authors": [
      "Yun Tian",
      "Zhijin Qin",
      "Guocheng Lv",
      "Ye Jin",
      "Kaibin Huang",
      "Zhu Han"
    ],
    "summary": "Existing speech semantic communication systems mainly based on Joint Source-Channel Coding (JSCC) architectures have demonstrated impressive performance, but their effectiveness remains limited by model structures specifically designed for particular tasks and datasets. Recent advances indicate that generative large models pre-trained on massive datasets, can achieve outstanding performance arexhibit exceptional performance across diverse downstream tasks with minimal fine-tuning. To exploit the rich semantic knowledge embedded in large models and enable adaptive transmission over lossy channels, we propose a Large Speech Model enabled Semantic Communication (LargeSC) system. Simultaneously achieving adaptive compression and robust transmission over lossy channels remains challenging, requiring trade-offs among compression efficiency, speech quality, and latency. In this work, we employ the Mimi as a speech codec, converting speech into discrete tokens compatible with existing network architectures. We propose an adaptive controller module that enables adaptive transmission and in-band Unequal Error Protection (UEP), dynamically adjusting to both speech content and packet loss probability under bandwidth constraints. Additionally, we employ Low-Rank Adaptation (LoRA) to finetune the Moshi foundation model for generative recovery of lost speech tokens. Simulation results show that the proposed system supports bandwidths ranging from 550 bps to 2.06 kbps, outperforms conventional baselines in speech quality under high packet loss rates and achieves an end-to-end latency of approximately 460 ms, thereby demonstrating its potential for real-time deployment.",
    "published": "2025-12-04",
    "categories": [
      "cs.SD",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04711v1"
  },
  {
    "id": "2512.04694v1",
    "title": "TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation",
    "authors": [
      "Baris Yilmaz",
      "Bevan Deniz Cilgin",
      "Erdem Akagündüz",
      "Salih Tileylioglu"
    ],
    "summary": "Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04694v1"
  },
  {
    "id": "2512.04691v1",
    "title": "Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective",
    "authors": [
      "Jae Hee Lee",
      "Anne Lauscher",
      "Stefano V. Albrecht"
    ],
    "summary": "Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04691v1"
  },
  {
    "id": "2512.04680v1",
    "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap",
    "authors": [
      "Jialong Li",
      "Mingyue Zhang",
      "Nianyu Li",
      "Danny Weyns",
      "Zhi Jin",
      "Kenji Tei"
    ],
    "summary": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.",
    "published": "2025-12-04",
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04680v1"
  },
  {
    "id": "2512.04668v1",
    "title": "Topology Matters: Measuring Memory Leakage in Multi-Agent LLMs",
    "authors": [
      "Jinbo Liu",
      "Defu Cao",
      "Yifei Wei",
      "Tianyao Su",
      "Yuan Liang",
      "Yushun Dong",
      "Yue Zhao",
      "Xiyang Hu"
    ],
    "summary": "Graph topology is a fundamental determinant of memory leakage in multi-agent LLM systems, yet its effects remain poorly quantified. We introduce MAMA (Multi-Agent Memory Attack), a framework that measures how network structure shapes leakage. MAMA operates on synthetic documents containing labeled Personally Identifiable Information (PII) entities, from which we generate sanitized task instructions. We execute a two-phase protocol: Engram (seeding private information into a target agent's memory) and Resonance (multi-round interaction where an attacker attempts extraction). Over up to 10 interaction rounds, we quantify leakage as the fraction of ground-truth PII recovered from attacking agent outputs via exact matching. We systematically evaluate six common network topologies (fully connected, ring, chain, binary tree, star, and star-ring), varying agent counts $n\\in\\{4,5,6\\}$, attacker-target placements, and base models. Our findings reveal consistent patterns: fully connected graphs exhibit maximum leakage while chains provide strongest protection; shorter attacker-target graph distance and higher target centrality significantly increase vulnerability; leakage rises sharply in early rounds before plateauing; model choice shifts absolute leakage rates but preserves topology rankings; temporal/locational PII attributes leak more readily than identity credentials or regulated identifiers. These results provide the first systematic mapping from architectural choices to measurable privacy risk, yielding actionable guidance: prefer sparse or hierarchical connectivity, maximize attacker-target separation, limit node degree and network radius, avoid shortcuts bypassing hubs, and implement topology-aware access controls.",
    "published": "2025-12-04",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04668v1"
  },
  {
    "id": "2512.04653v1",
    "title": "Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control",
    "authors": [
      "Pouria Yazdani",
      "Arash Rezaali",
      "Monireh Abdoos"
    ],
    "summary": "Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.",
    "published": "2025-12-04",
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04653v1"
  },
  {
    "id": "2512.04643v1",
    "title": "SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding",
    "authors": [
      "Chang-Hsun Wu",
      "Kai-Po Chang",
      "Yu-Yang Sheng",
      "Hung-Kai Chung",
      "Kuei-Chun Wang",
      "Yu-Chiang Frank Wang"
    ],
    "summary": "Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04643v1"
  },
  {
    "id": "2512.04639v1",
    "title": "When GenAI Meets Fake News: Understanding Image Cascade Dynamics on Reddit",
    "authors": [
      "Saumya Chauhan",
      "Mila Hong",
      "Maria Vazhaeparambil"
    ],
    "summary": "AI-generated content and misinformation are increasingly prevalent on social networks. While prior research primarily examined textual misinformation, fewer studies have focused on visual content's role in virality. In this work, we present the first large-scale analysis of how misinformation and AI-generated images propagate through repost cascades across five ideologically diverse Reddit communities. By integrating textual sentiment, visual attributes, and diffusion metrics (e.g., time-to-first repost, community reach), our framework accurately predicts both immediate post-level virality (AUC=0.83) and long-term cascade-level spread (AUC=0.998). These findings offer essential insights for moderating synthetic and misleading visual content online.",
    "published": "2025-12-04",
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04639v1"
  },
  {
    "id": "2512.04632v1",
    "title": "Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning",
    "authors": [
      "Thibaut Boissin",
      "Thomas Massena",
      "Franck Mamalet",
      "Mathieu Serrurier"
    ],
    "summary": "Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04632v1"
  },
  {
    "id": "2512.04629v1",
    "title": "BioMedGPT-Mol: Multi-task Learning for Molecular Understanding and Generation",
    "authors": [
      "Chenyang Zuo",
      "Siqi Fan",
      "Zaiqing Nie"
    ],
    "summary": "Molecules play a crucial role in biomedical research and discovery, particularly in the field of small molecule drug development. Given the rapid advancements in large language models, especially the recent emergence of reasoning models, it is natural to explore how a general-purpose language model can be efficiently adapted for molecular science applications. In this work, we introduce BioMedGPT-Mol, a molecular language model designed to support molecular understanding and generation tasks. By curating and unifying existing public instruction datasets, we have assembled a large-scale, comprehensive, and high-quality training dataset. The model is then fine-tuned through a meticulously designed multi-task learning framework. On a consolidated benchmark derived from LlaSMol, TOMG-Bench, and MuMOInstruct, BioMedGPT-Mol achieves remarkable performance. Our experimental results demonstrate that a general-purpose reasoning model can be effectively and efficiently post-trained into a professional molecular language model through a well-structured multi-task curriculum. Leveraging the power of it, we further explore retrosynthetic planning task, and the performance on RetroBench demonstrates its competitive capability of acting as an end-to-end retrosynthetic planner. We anticipate that our approach can be extended to other biomedical scientific domains.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04629v1"
  },
  {
    "id": "2512.04618v1",
    "title": "Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning",
    "authors": [
      "Mohamed Baha Ben Ticha",
      "Xingchen Ran",
      "Guillaume Saldanha",
      "Gaël Le Godais",
      "Philémon Roussel",
      "Marc Aubert",
      "Amina Fontanell",
      "Thomas Costecalde",
      "Lucas Struber",
      "Serpil Karakas",
      "Shaomin Zhang",
      "Philippe Kahane",
      "Guillaume Charvet",
      "Stéphan Chabardès",
      "Blaise Yvert"
    ],
    "summary": "Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04618v1"
  },
  {
    "id": "2512.04598v1",
    "title": "The Ethics of Generative AI",
    "authors": [
      "Michael Klenk"
    ],
    "summary": "This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04598v1"
  },
  {
    "id": "2512.04597v1",
    "title": "When Robots Should Say \"I Don't Know\": Benchmarking Abstention in Embodied Question Answering",
    "authors": [
      "Tao Wu",
      "Chuhao Zhou",
      "Guangyu Zhao",
      "Haozhi Cao",
      "Yewen Pu",
      "Jianfei Yang"
    ],
    "summary": "Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04597v1"
  },
  {
    "id": "2512.04580v1",
    "title": "A Light-Weight Large Language Model File Format for Highly-Secure Model Distribution",
    "authors": [
      "Huifeng Zhu",
      "Shijie Li",
      "Qinfeng Li",
      "Yier Jin"
    ],
    "summary": "To enhance the performance of large language models (LLMs) in various domain-specific applications, sensitive data such as healthcare, law, and finance are being used to privately customize or fine-tune these models. Such privately adapted LLMs are regarded as either personal privacy assets or corporate intellectual property. Therefore, protecting model weights and maintaining strict confidentiality during deployment and distribution have become critically important. However, existing model formats and deployment frameworks provide little to no built-in support for confidentiality, access control, or secure integration with trusted hardware. Current methods for securing model deployment either rely on computationally expensive cryptographic techniques or tightly controlled private infrastructure. Although these approaches can be effective in specific scenarios, they are difficult and costly for widespread deployment.\n  In this paper, we introduce CryptoTensors, a secure and format-compatible file structure for confidential LLM distribution. Built as an extension to the widely adopted Safetensors format, CryptoTensors incorporates tensor-level encryption and embedded access control policies, while preserving critical features such as lazy loading and partial deserialization. It enables transparent decryption and automated key management, supporting flexible licensing and secure model execution with minimal overhead. We implement a proof-of-concept library, benchmark its performance across serialization and runtime scenarios, and validate its compatibility with existing inference frameworks, including Hugging Face Transformers and vLLM. Our results highlight CryptoTensors as a light-weight, efficient, and developer-friendly solution for safeguarding LLM weights in real-world and widespread deployments.",
    "published": "2025-12-04",
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04580v1"
  },
  {
    "id": "2512.04559v1",
    "title": "Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function",
    "authors": [
      "Hyeongyu Kang",
      "Jaewoo Lee",
      "Woocheol Shin",
      "Kiyoung Om",
      "Jinkyoo Park"
    ],
    "summary": "Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \\textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04559v1"
  },
  {
    "id": "2512.04552v1",
    "title": "RRPO: Robust Reward Policy Optimization for LLM-based Emotional TTS",
    "authors": [
      "Cong Wang",
      "Changfeng Gao",
      "Yang Xiang",
      "Zhihao Du",
      "Keyu An",
      "Han Zhao",
      "Qian Chen",
      "Xiangang Li",
      "Yingming Gao",
      "Ya Li"
    ],
    "summary": "Differentiable reinforcement learning (RL) frameworks like DiffRO offer a powerful approach for controllable text-to-speech (TTS), but are vulnerable to reward hacking, particularly for nuanced tasks like emotion control. The policy model can exploit a vanilla Reward Model (RM) by generating acoustic artifacts to achieve spurious rewards, but at the cost of degrading perceptual quality. To address this, we propose Robust Reward Policy Optimization (RRPO), a novel framework that employs a hybrid regularization scheme. This scheme develops a robust RM whose reward signal is more reliably aligned with human perception, compelling the policy to abandon detrimental shortcuts and instead learn the complex features of genuine emotions. Our ablation study confirms the enhanced robustness of our RM, as evidenced by its strong cross-lingual generalization. The subjective evaluation demonstrates that this robust RM effectively mitigates reward hacking, leading to significant improvements in both emotional expressiveness and naturalness over all baselines. Demo page: https://lrwinr.github.io/RRPO-CosyVoice.",
    "published": "2025-12-04",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04552v1"
  },
  {
    "id": "2512.04551v1",
    "title": "Multi-Loss Learning for Speech Emotion Recognition with Energy-Adaptive Mixup and Frame-Level Attention",
    "authors": [
      "Cong Wang",
      "Yizhong Geng",
      "Yuhua Wen",
      "Qifei Li",
      "Yingming Gao",
      "Ruimin Wang",
      "Chunfeng Wang",
      "Hao Li",
      "Ya Li",
      "Wei Chen"
    ],
    "summary": "Speech emotion recognition (SER) is an important technology in human-computer interaction. However, achieving high performance is challenging due to emotional complexity and scarce annotated data. To tackle these challenges, we propose a multi-loss learning (MLL) framework integrating an energy-adaptive mixup (EAM) method and a frame-level attention module (FLAM). The EAM method leverages SNR-based augmentation to generate diverse speech samples capturing subtle emotional variations. FLAM enhances frame-level feature extraction for multi-frame emotional cues. Our MLL strategy combines Kullback-Leibler divergence, focal, center, and supervised contrastive loss to optimize learning, address class imbalance, and improve feature separability. We evaluate our method on four widely used SER datasets: IEMOCAP, MSP-IMPROV, RAVDESS, and SAVEE. The results demonstrate our method achieves state-of-the-art performance, suggesting its effectiveness and robustness.",
    "published": "2025-12-04",
    "categories": [
      "cs.SD",
      "cs.AI",
      "eess.AS"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04551v1"
  },
  {
    "id": "2512.04550v1",
    "title": "AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees",
    "authors": [
      "Yangning Li",
      "Shaoshen Chen",
      "Yinghui Li",
      "Yankai Chen",
      "Hai-Tao Zheng",
      "Hui Wang",
      "Wenhao Jiang",
      "Philip S. Yu"
    ],
    "summary": "The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04550v1"
  },
  {
    "id": "2512.04536v1",
    "title": "Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model",
    "authors": [
      "Bita Baroutian",
      "Atefe Aghaei",
      "Mohsen Ebrahimi Moghaddam"
    ],
    "summary": "Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04536v1"
  },
  {
    "id": "2512.04535v1",
    "title": "GTM: Simulating the World of Tools for AI Agents",
    "authors": [
      "Zhenzhen Ren",
      "Xinpeng Zhang",
      "Zhenxing Qian",
      "Yan Gao",
      "Yu Shi",
      "Shuxin Zheng",
      "Jiyan He"
    ],
    "summary": "The integration of external tools is pivotal for empowering Large Language Model (LLM) agents with real-world capabilities. However, training these agents through direct, continuous interaction with diverse tools is often prohibitively expensive, slow, and introduces additional development and maintenance overhead. To address this challenge, we introduce the Generalist Tool Model (GTM), a 1.5-billion-parameter model that learns to act as a universal tool simulator. With only prompt-level configuration, GTM accesses tool functionalities along with input arguments and generates outputs that faithfully mimic real tool execution, providing a fast and cost-effective solution that eliminates development overhead. To build GTM, we propose the Context-Aware Response Generation (CARG) pipeline, which synthesizes comprehensive training data covering over 20,000 tools across 300 domains including physics, medicine, robotics, and finance. Through this pipeline, GTM learns to produce not only syntactically correct outputs but also logically coherent and contextually appropriate responses. Experiments demonstrate that GTM produces high-quality outputs with strong consistency and reliability. Besides when used in real reinforcement learning scenarios for agent training, GTM exhibits significantly faster simulation speed compared to real tools while maintaining comparable output quality, along with remarkable generalization and domain adaptability. Our results establish GTM as a foundational component for developing future AI agents, enabling efficient and scalable training of tool-augmented systems.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04535v1"
  },
  {
    "id": "2512.04532v1",
    "title": "PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement",
    "authors": [
      "Yu-Wei Zhan",
      "Xin Wang",
      "Hong Chen",
      "Tongtong Feng",
      "Wei Feng",
      "Ren Wang",
      "Guangyao Li",
      "Qing Li",
      "Wenwu Zhu"
    ],
    "summary": "Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04532v1"
  },
  {
    "id": "2512.04529v1",
    "title": "SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation",
    "authors": [
      "Xin Liang",
      "Xiang Zhang",
      "Yiwei Xu",
      "Siqi Sun",
      "Chenyu You"
    ],
    "summary": "Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04529v1"
  },
  {
    "id": "2512.04524v1",
    "title": "Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval",
    "authors": [
      "Tianle Hu",
      "Weijun Lv",
      "Na Han",
      "Xiaozhao Fang",
      "Jie Wen",
      "Jiaxing Li",
      "Guoxu Zhou"
    ],
    "summary": "Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04524v1"
  },
  {
    "id": "2512.04518v1",
    "title": "UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction",
    "authors": [
      "Tianmai M. Zhang",
      "Zhaoyi Sun",
      "Sihang Zeng",
      "Chenxi Li",
      "Neil F. Abernethy",
      "Barbara D. Lam",
      "Fei Xia",
      "Meliha Yetisgen"
    ],
    "summary": "The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04518v1"
  },
  {
    "id": "2512.04513v1",
    "title": "BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models",
    "authors": [
      "Yu-Wei Zhan",
      "Xin Wang",
      "Pengzhe Mao",
      "Tongtong Feng",
      "Ren Wang",
      "Wenwu Zhu"
    ],
    "summary": "Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04513v1"
  },
  {
    "id": "2512.04500v1",
    "title": "A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework",
    "authors": [
      "Edervaldo Melo"
    ],
    "summary": "This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules (\"personas\") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04500v1"
  },
  {
    "id": "2512.04488v1",
    "title": "Persona-based Multi-Agent Collaboration for Brainstorming",
    "authors": [
      "Nate Straub",
      "Saara Khan",
      "Katharina Jay",
      "Brian Cabral",
      "Oskar Linde"
    ],
    "summary": "We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04488v1"
  },
  {
    "id": "2512.04480v1",
    "title": "AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions",
    "authors": [
      "Pedro Passos"
    ],
    "summary": "In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the \"FAGNER Paradox\" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the \"Lukaku Paradox\", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.CE",
      "eess.SY",
      "math.OC"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04480v1"
  },
  {
    "id": "2512.04475v1",
    "title": "GraphBench: Next-generation graph learning benchmarking",
    "authors": [
      "Timo Stoll",
      "Chendi Qian",
      "Ben Finkelshtein",
      "Ali Parviz",
      "Darius Weber",
      "Fabrizio Frasca",
      "Hadar Shavit",
      "Antoine Siraudin",
      "Arman Mielke",
      "Marie Anastacio",
      "Erik Müller",
      "Maya Bechler-Speicher",
      "Michael Bronstein",
      "Mikhail Galkin",
      "Holger Hoos",
      "Mathias Niepert",
      "Bryan Perozzi",
      "Jan Tönshoff",
      "Christopher Morris"
    ],
    "summary": "Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE",
      "stat.ML"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04475v1"
  },
  {
    "id": "2512.04469v1",
    "title": "Mathematical Framing for Different Agent Strategies",
    "authors": [
      "Philip Stephens",
      "Emmanuel Salawu"
    ],
    "summary": "We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the \"Degrees of Freedom\" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04469v1"
  },
  {
    "id": "2512.04463v1",
    "title": "MARL Warehouse Robots",
    "authors": [
      "Price Allman",
      "Lian Thang",
      "Dre Simmons",
      "Salmon Riaz"
    ],
    "summary": "We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04463v1"
  },
  {
    "id": "2512.04456v1",
    "title": "GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis",
    "authors": [
      "Changjin Kim",
      "HyeokJun Lee",
      "YoungJoon Yoo"
    ],
    "summary": "Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04456v1"
  },
  {
    "id": "2512.04453v1",
    "title": "Open-Ended Goal Inference through Actions and Language for Human-Robot Collaboration",
    "authors": [
      "Debasmita Ghose",
      "Oz Gitelson",
      "Marynel Vazquez",
      "Brian Scassellati"
    ],
    "summary": "To collaborate with humans, robots must infer goals that are often ambiguous, difficult to articulate, or not drawn from a fixed set. Prior approaches restrict inference to a predefined goal set, rely only on observed actions, or depend exclusively on explicit instructions, making them brittle in real-world interactions. We present BALI (Bidirectional Action-Language Inference) for goal prediction, a method that integrates natural language preferences with observed human actions in a receding-horizon planning tree. BALI combines language and action cues from the human, asks clarifying questions only when the expected information gain from the answer outweighs the cost of interruption, and selects supportive actions that align with inferred goals. We evaluate the approach in collaborative cooking tasks, where goals may be novel to the robot and unbounded. Compared to baselines, BALI yields more stable goal predictions and significantly fewer mistakes.",
    "published": "2025-12-04",
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04453v1"
  },
  {
    "id": "2512.04452v1",
    "title": "NORi: An ML-Augmented Ocean Boundary Layer Parameterization",
    "authors": [
      "Xin Kai Lee",
      "Ali Ramadhan",
      "Andre Souza",
      "Gregory LeClaire Wagner",
      "Simone Silvestri",
      "John Marshall",
      "Raffaele Ferrari"
    ],
    "summary": "NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an \"a posteriori\" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.",
    "published": "2025-12-04",
    "categories": [
      "physics.ao-ph",
      "cs.AI",
      "cs.LG",
      "physics.comp-ph",
      "physics.flu-dyn"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04452v1"
  },
  {
    "id": "2512.04445v1",
    "title": "Automating Complex Document Workflows via Stepwise and Rollback-Enabled Operation Orchestration",
    "authors": [
      "Yanbin Zhang",
      "Hanhui Ye",
      "Yue Bai",
      "Qiming Zhang",
      "Liao Xiang",
      "Wu Mianzhi",
      "Renjun Hu"
    ],
    "summary": "Workflow automation promises substantial productivity gains in everyday document-related tasks. While prior agentic systems can execute isolated instructions, they struggle with automating multi-step, session-level workflows due to limited control over the operational process. To this end, we introduce AutoDW, a novel execution framework that enables stepwise, rollback-enabled operation orchestration. AutoDW incrementally plans API actions conditioned on user instructions, intent-filtered API candidates, and the evolving states of the document. It further employs robust rollback mechanisms at both the argument and API levels, enabling dynamic correction and fault tolerance. These designs together ensure that the execution trajectory of AutoDW remains aligned with user intent and document context across long-horizon workflows. To assess its effectiveness, we construct a comprehensive benchmark of 250 sessions and 1,708 human-annotated instructions, reflecting realistic document processing scenarios with interdependent instructions. AutoDW achieves 90% and 62% completion rates on instruction- and session-level tasks, respectively, outperforming strong baselines by 40% and 76%. Moreover, AutoDW also remains robust for the decision of backbone LLMs and on tasks with varying difficulty. Code and data will be open-sourced. Code: https://github.com/YJett/AutoDW",
    "published": "2025-12-04",
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04445v1"
  },
  {
    "id": "2512.04442v1",
    "title": "TaskEval: Synthesised Evaluation for Foundation-Model Tasks",
    "authors": [
      "Dilani Widanapathiranage",
      "Scott Barnett",
      "Stefanus Kurniawan",
      "Wannita Takerngsaksiri"
    ],
    "summary": "Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \\textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \\toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\\% and 90\\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04442v1"
  },
  {
    "id": "2512.04425v1",
    "title": "Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models",
    "authors": [
      "Manar Alnaasan",
      "Md Selim Sarowar",
      "Sungho Kim"
    ],
    "summary": "Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04425v1"
  },
  {
    "id": "2512.04419v1",
    "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions",
    "authors": [
      "Weiwei Wang",
      "Weijie Zou",
      "Jiyong Min"
    ],
    "summary": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.\n  We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.\n  Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.\n  The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04419v1"
  },
  {
    "id": "2512.04416v1",
    "title": "GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows",
    "authors": [
      "Zhou Liu",
      "Zhaoyang Han",
      "Guochen Yan",
      "Hao Liang",
      "Bohan Zeng",
      "Xing Chen",
      "Yuanfeng Song",
      "Wentao Zhang"
    ],
    "summary": "Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel \"reversed-objective\" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04416v1"
  },
  {
    "id": "2512.04413v1",
    "title": "Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection",
    "authors": [
      "Xiangyi Gao",
      "Danpei Zhao",
      "Bo Yuan",
      "Wentao Li"
    ],
    "summary": "Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04413v1"
  },
  {
    "id": "2512.04408v1",
    "title": "Executable Governance for AI: Translating Policies into Rules Using LLMs",
    "authors": [
      "Gautam Varma Datla",
      "Anudeep Vurity",
      "Tejaswani Dash",
      "Tazeem Ahmad",
      "Mohd Adnan",
      "Saima Rafi"
    ],
    "summary": "AI policy guidance is predominantly written as prose, which practitioners must first convert into executable rules before frameworks can evaluate or enforce them. This manual step is slow, error-prone, difficult to scale, and often delays the use of safeguards in real-world deployments. To address this gap, we present Policy-to-Tests (P2T), a framework that converts natural-language policy documents into normalized, machine-readable rules. The framework comprises a pipeline and a compact domain-specific language (DSL) that encodes hazards, scope, conditions, exceptions, and required evidence, yielding a canonical representation of extracted rules. To test the framework beyond a single policy, we apply it across general frameworks, sector guidance, and enterprise standards, extracting obligation-bearing clauses and converting them into executable rules. These AI-generated rules closely match strong human baselines on span-level and rule-level metrics, with robust inter-annotator agreement on the gold set. To evaluate downstream behavioral and safety impact, we add HIPAA-derived safeguards to a generative agent and compare it with an otherwise identical agent without guardrails. An LLM-based judge, aligned with gold-standard criteria, measures violation rates and robustness to obfuscated and compositional prompts. Detailed results are provided in the appendix. We release the codebase, DSL, prompts, and rule sets as open-source resources to enable reproducible evaluation.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04408v1"
  },
  {
    "id": "2512.04405v1",
    "title": "Towards 6G Native-AI Edge Networks: A Semantic-Aware and Agentic Intelligence Paradigm",
    "authors": [
      "Chenyuan Feng",
      "Anbang Zhang",
      "Geyong Min",
      "Yongming Huang",
      "Tony Q. S. Quek",
      "Xiaohu You"
    ],
    "summary": "The evolution toward sixth-generation wireless systems positions intelligence as a native network capability, fundamentally transforming the design of radio access networks (RANs). Within this vision, Semantic-native communication and agentic intelligence are expected to play central roles. SemCom departs from bit-level fidelity and instead emphasizes task-oriented meaning exchange, enabling compact SC and introducing new performance measures such as semantic fidelity and task success rate. Agentic intelligence endows distributed RAN entities with goal-driven autonomy, reasoning, planning, and multi-agent collaboration, increasingly supported by foundation models and knowledge graphs. In this work, we first introduce the conceptual foundations of SemCom and agentic networking, and discuss why existing AI-driven O-RAN solutions remain largely bit-centric and task-siloed. We then present a unified taxonomy that organizes recent research along three axes: i) semantic abstraction level (symbol/feature/intent/knowledge), ii) agent autonomy and coordination granularity (single-, multi-, and hierarchical-agent), and iii) RAN control placement across PHY/MAC, near-real-time RIC, and non-real-time RIC. Based on this taxonomy, we systematically introduce enabling technologies including task-oriented semantic encoders/decoders, multi-agent reinforcement learning, foundation-model-assisted RAN agents, and knowledge-graph-based reasoning for cross-layer awareness. Representative 6G use cases, such as immersive XR, vehicular V2X, and industrial digital twins, are analyzed to illustrate the semantic-agentic convergence in practice. Finally, we identify open challenges in semantic representation standardization, scalable trustworthy agent coordination, O-RAN interoperability, and energy-efficient AI deployment, and outline research directions toward operational semantic-agentic AI-RAN.",
    "published": "2025-12-04",
    "categories": [
      "eess.SP",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04405v1"
  },
  {
    "id": "2512.04391v1",
    "title": "Adversarial Limits of Quantum Certification: When Eve Defeats Detection",
    "authors": [
      "Davut Emre Tasar"
    ],
    "summary": "Security of quantum key distribution (QKD) relies on certifying that observed correlations arise from genuine quantum entanglement rather than eavesdropper manipulation. Theoretical security proofs assume idealized conditions, practical certification must contend with adaptive adversaries who optimize their attack strategies against detection systems. Established fundamental adversarial limits for quantum certification using Eve GAN, a generative adversarial network trained to produce classical correlations indistinguishable from quantum. Our central finding: when Eve interpolates her classical correlations with quantum data at mixing parameter, all tested detection methods achieve ROC AUC = 0.50, equivalent to random guessing. This means an eavesdropper needs only 5% classical admixture to completely evade detection. Critically, we discover that same distribution calibration a common practice in prior certification studies inflates detection performance by 44 percentage points compared to proper cross distribution evaluation, revealing a systematic flaw that may have led to overestimated security claims. Analysis of Popescu Rohrlich (PR Box) regime identifies a sharp phase transition at CHSH S = 2.05: below this value, no statistical method distinguishes classical from quantum correlations; above it, detection probability increases monotonically. Hardware validation on IBM Quantum demonstrates that Eve-GAN achieves CHSH = 2.736, remarkably exceeding real quantum hardware performance (CHSH = 2.691), illustrating that classical adversaries can outperform noisy quantum systems on standard certification metrics. These results have immediate implications for QKD security: adversaries maintaining 95% quantum fidelity evade all tested detection methods. We provide corrected methodology using cross-distribution calibration and recommend mandatory adversarial testing for quantum security claims.",
    "published": "2025-12-04",
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04391v1"
  },
  {
    "id": "2512.04390v1",
    "title": "FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring",
    "authors": [
      "Geunhyuk Youk",
      "Jihyong Oh",
      "Munchurl Kim"
    ],
    "summary": "Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04390v1"
  },
  {
    "id": "2512.04386v1",
    "title": "MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation",
    "authors": [
      "Zhou Yang",
      "Shunyan Luo",
      "Jiazhen Zhu",
      "Fang Jin"
    ],
    "summary": "Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.",
    "published": "2025-12-04",
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04386v1"
  },
  {
    "id": "2512.04385v1",
    "title": "STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting",
    "authors": [
      "Nan Zhou",
      "Weijie Hong",
      "Huandong Wang",
      "Jianfeng Zheng",
      "Qiuhua Wang",
      "Yali Song",
      "Xiao-Ping Zhang",
      "Yong Li",
      "Xinlei Chen"
    ],
    "summary": "Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.",
    "published": "2025-12-04",
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04385v1"
  },
  {
    "id": "2512.04368v1",
    "title": "AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning",
    "authors": [
      "Praveen Anugula",
      "Avdhesh Kumar Bhardwaj",
      "Navin Chhibber",
      "Rohit Tewari",
      "Sunil Khemka",
      "Piyush Ranjan"
    ],
    "summary": "Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods.\n  Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation",
    "published": "2025-12-04",
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04368v1"
  },
  {
    "id": "2512.04367v1",
    "title": "AgentBay: A Hybrid Interaction Sandbox for Seamless Human-AI Intervention in Agentic Systems",
    "authors": [
      "Yun Piao",
      "Hongbo Min",
      "Hang Su",
      "Leilei Zhang",
      "Lei Wang",
      "Yue Yin",
      "Xiao Wu",
      "Zhejing Xu",
      "Liwei Qu",
      "Hang Li",
      "Xinxin Zeng",
      "Wei Tian",
      "Fei Yu",
      "Xiaowei Li",
      "Jiayi Jiang",
      "Tongxu Liu",
      "Hao Tian",
      "Yufei Que",
      "Xiaobing Tu",
      "Bing Suo",
      "Yuebing Li",
      "Xiangting Chen",
      "Zeen Zhao",
      "Jiaming Tang",
      "Wei Huang",
      "Xuguang Li",
      "Jing Zhao",
      "Jin Li",
      "Jie Shen",
      "Jinkui Ren",
      "Xiantao Zhang"
    ],
    "summary": "The rapid advancement of Large Language Models (LLMs) is catalyzing a shift towards autonomous AI Agents capable of executing complex, multi-step tasks. However, these agents remain brittle when faced with real-world exceptions, making Human-in-the-Loop (HITL) supervision essential for mission-critical applications. In this paper, we present AgentBay, a novel sandbox service designed from the ground up for hybrid interaction. AgentBay provides secure, isolated execution environments spanning Windows, Linux, Android, Web Browsers, and Code interpreters. Its core contribution is a unified session accessible via a hybrid control interface: An AI agent can interact programmatically via mainstream interfaces (MCP, Open Source SDK), while a human operator can, at any moment, seamlessly take over full manual control. This seamless intervention is enabled by Adaptive Streaming Protocol (ASP). Unlike traditional VNC/RDP, ASP is specifically engineered for this hybrid use case, delivering an ultra-low-latency, smoother user experience that remains resilient even in weak network environments. It achieves this by dynamically blending command-based and video-based streaming, adapting its encoding strategy based on network conditions and the current controller (AI or human). Our evaluation demonstrates strong results in security, performance, and task completion rates. In a benchmark of complex tasks, the AgentBay (Agent + Human) model achieved more than 48% success rate improvement. Furthermore, our ASP protocol reduces bandwidth consumption by up to 50% compared to standard RDP, and in end-to-end latency with around 5% reduction, especially under poor network conditions. We posit that AgentBay provides a foundational primitive for building the next generation of reliable, human-supervised autonomous systems.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04367v1"
  },
  {
    "id": "2512.04359v1",
    "title": "Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning",
    "authors": [
      "Hongye Cao",
      "Zhixin Bai",
      "Ziyue Peng",
      "Boyan Wang",
      "Tianpei Yang",
      "Jing Huo",
      "Yuyao Zhang",
      "Yang Gao"
    ],
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.",
    "published": "2025-12-04",
    "categories": [
      "cs.AI"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04359v1"
  },
  {
    "id": "2512.04356v1",
    "title": "Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment",
    "authors": [
      "Kai-Po Chang",
      "Wei-Yuan Cheng",
      "Chi-Pin Huang",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ],
    "summary": "Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.",
    "published": "2025-12-04",
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04356v1"
  },
  {
    "id": "2512.04355v1",
    "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity",
    "authors": [
      "Gregory Bolet",
      "Giorgis Georgakoudis",
      "Konstantinos Parasyris",
      "Harshitha Menon",
      "Niranjan Hasabnis",
      "Kirk W. Cameron",
      "Gal Oren"
    ],
    "summary": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench",
    "published": "2025-12-04",
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF"
    ],
    "pdf_url": "https://arxiv.org/pdf/2512.04355v1"
  }
]